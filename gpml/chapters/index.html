<html>
  <head>
    <title>Gaussian Processes for Machine Learning: Contents</title>

  <style type="text/css">
    body {font-family: Verdana,Arial,Helvetica,sans-serif; font-size: 16px}
  </style>


</head>

<body>
<center>
<h2>Gaussian Processes for Machine Learning</h2>
Carl Edward Rasmussen and Christopher K. I. Williams<br>
MIT Press, 2006. ISBN-10 0-262-18253-X, ISBN-13 978-0-262-18253-9.
</center>

<p>This <a href="https://mitpress.mit.edu/books/gaussian-processes-machine-learning">book</a>
is &copy; Copyright 2006 by Massachusetts Institute of Technology.
The <a href="http://mitpress.mit.edu">MIT Press</a> have kindly
agreed to allow us to make the book available on the web. The web version of
the book corresponds to the 2nd printing. You can <a href="../order.html">buy
the book</a> for a list price of 50.00 US&#36; or 23.95 UK&pound;.</p>

<p>The whole <a href="RW.pdf">book</a> as a single pdf file.</p>

<h3>List of contents and individual chapters in pdf format</h3>

<p>
<dl>
<dt> <a href="RW0.pdf">Frontmatter</a>
<dd> Table of Contents 
<dd> Series Foreword 
<dd> Preface 
<dd> Symbols and Notation 
<dt> 1 <a href="RW1.pdf">Introduction</a> 
<dd> 1.1 A Pictorial Introduction to Bayesian Modelling
<dd> 1.2 Roadmap
<dt> 2 <a href="RW2.pdf">Regression</a> 
<dd> 2.1 Weight-space View
<dd> 2.2 Function-space View
<dd> 2.3 Varying the Hyperparameters
<dd> 2.4 Decision Theory for Regression
<dd> 2.5 An Example Application
<dd> 2.6 Smoothing, Weight Functions and Equivalent Kernels
<dd> 2.7 History and Related Work
<dd> 2.8 Appendix: Infinite Radial Basis Function Networks
<dd> 2.9 Exercises
<dt> 3 <a href="RW3.pdf">Classification</a> 
<dd> 3.1 Classification Problems
<dd> 3.2 Linear Models for Classification
<dd> 3.3 Gaussian Process Classification
<dd> 3.4 The Laplace Approximation for the Binary GP Classifier
<dd> 3.5 Multi-class Laplace Approximation
<dd> 3.6 Expectation Propagation
<dd> 3.7 Experiments
<dd> 3.8 Discussion
<dd> 3.9 Appendix: Moment Derivations
<dd> 3.10 Exercises
<dt> 4 <a href="RW4.pdf">Covariance Functions</a> 
<dd> 4.1 Preliminaries 
<dd> 4.2 Examples of Covariance Functions
<dd> 4.3 Eigenfunction Analysis of Kernels
<dd> 4.4 Kernels for Non-vectorial Inputs
<dd> 4.5 Exercises
<dt> 5 <a href="RW5.pdf">Model Selection and Adaptation of Hyperparameters</a> 
<dd> 5.1 The Model Selection Problem 
<dd> 5.2 Bayesian Model Selection
<dd> 5.3 Cross-validation
<dd> 5.4 Model Selection for GP Regression
<dd> 5.5 Model Selection for GP Classification
<dd> 5.6 Exercises    
<dt> 6 <a href="RW6.pdf">Relationships between GPs and Other Models</a> 
<dd> 6.1 Reproducing Kernel Hilbert Spaces
<dd> 6.2 Regularization 
<dd> 6.3 Spline Models
<dd> 6.4 Support Vector Machines
<dd> 6.5 Least-Squares Classification
<dd> 6.6 Relevance Vector Machines
<dd> 6.7 Exercises 
<dt> 7 <a href="RW7.pdf">Theoretical Perspectives</a> 
<dd> 7.1 The Equivalent Kernel
<dd> 7.2 Asymptotic Analysis
<dd> 7.3 Average-case Learning Curves
<dd> 7.4 PAC-Bayesian Analysis
<dd> 7.5 Comparison with Other Supervised Learning Methods
<dd> 7.6 Appendix: Learning Curve for the Ornstein-Uhlenbeck Process 
<dd> 7.7 Exercises 
<dt> 8 <a href="RW8.pdf">Approximation Methods for Large Datasets</a> 
<dd> 8.1 Reduced-rank Approximations of the Gram Matrix
<dd> 8.2 Greedy Approximation
<dd> 8.3 Approximations for GPR with Fixed Hyperparameters 
<dd> 8.4 Approximations for GPC with Fixed Hyperparameters
<dd> 8.5 Approximating the Marginal Likelihood and its Derivatives
<dd> 8.6 Appendix: Equivalence of SR and GPR using the Nystr&ouml;m
Approximate Kernel
<dd> 8.7 Exercises 
<dt> 9 <a href="RW9.pdf">Further Issues and Conclusions</a> 
<dd> 9.1 Multiple Outputs
<dd> 9.2 Noise Models with Dependencies
<dd> 9.3 Non-Gaussian Likelihoods
<dd> 9.4 Derivative Observations
<dd> 9.5 Prediction with Uncertain Inputs 
<dd> 9.6 Mixtures of Gaussian Processes
<dd> 9.7 Global Optimization
<dd> 9.8 Evaluation of Integrals 
<dd> 9.9 Student's t Process
<dd> 9.10 Invariances
<dd> 9.11 Latent Variable Models
<dd> 9.12 Conclusions and Future Directions 
<dt> A <a href="RWA.pdf">Mathematical Background</a> 
<dt> B <a href="RWB.pdf">Gaussian Markov Processes</a> 
<dt> C <a href="RWC.pdf">Datasets and Code 
<dt> <a href="RWDbib.pdf">Bibliography</a> 
<dt> <a href="RWEaidx.pdf">Author Index</a> 
<dt> <a href="RWFsidx.pdf">Subject Index</a> 
</dl>
</p>

<br>

Go back to the <a href="..">web page</a> for Gaussian Processes for Machine
Learning.

<br>

<hr>

<table width="100%"><tr><td>
This page was most recently updated by <a href="http://mlg.eng.cam.ac.uk/carl">Carl Edward Rasmussen</a> on April 1st, 2022.</td>

<td align="right>
<div id="eXTReMe"><a href="http://extremetracking.com/open?login=gpmlchap">
<img src="http://t1.extreme-dm.com/i.gif" style="border: 0;"
height="38" width="41" id="EXim" alt="eXTReMe Tracker" /></a>
<script type="text/javascript"><!--
var EXlogin='gpmlchap' // Login
var EXvsrv='s10' // VServer
EXs=screen;EXw=EXs.width;navigator.appName!="Netscape"?
EXb=EXs.colorDepth:EXb=EXs.pixelDepth;
navigator.javaEnabled()==1?EXjv="y":EXjv="n";
EXd=document;EXw?"":EXw="na";EXb?"":EXb="na";
EXd.write("<img src=http://e1.extreme-dm.com",
"/"+EXvsrv+".g?login="+EXlogin+"&amp;",
"jv="+EXjv+"&amp;j=y&amp;srw="+EXw+"&amp;srb="+EXb+"&amp;",
"l="+escape(EXd.referrer)+" height=1 width=1>");//-->
</script><noscript><div id="neXTReMe"><img height="1" width="1" alt=""
src="http://e1.extreme-dm.com/s10.g?login=gpmlchap&amp;j=n&amp;jv=n" />
</div></noscript></div>

</td></tr></table>

</body>
</html>
