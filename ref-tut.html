
<p id="mackay-03">
D.&nbsp;J.&nbsp;C. MacKay.
<a href="http://www.inference.phy.cam.ac.uk/mackay/itila"><b>Information
  Theory, Inference and Learning Algorithms</b></a>.
Cambridge University Press, Cambridge, UK, 2003.
<a href="http://www.inference.phy.cam.ac.uk/mackay/itprnn/ps/534.548.pdf">chapter
  45</a>.

<p class="c"><b> Comment:</b> A short introduction to GPs, emphasizing the
  relationships to paramteric models (RBF networks, neural networks,
  splines).</p>

</p>

<p id="mackay-97">
D.&nbsp;J.&nbsp;C. MacKay.
<a href="http://wol.ra.phy.cam.ac.uk/mackay/abstracts/gp.html"><b>Gaussian
  processes - a replacement for supervised neural networks?</b></a>.
Tutorial lecture notes for NIPS 1997, 1997.



</p>

<p id="mackay-98">
D.&nbsp;J.&nbsp;C. MacKay.
<a href="http://www.inference.phy.cam.ac.uk/mackay/gpB.pdf"><b>Introduction to
  Gaussian processes</b></a>.
In C.&nbsp;M. Bishop, editor, <em>Neural Networks and Machine Learning</em>, volume
  168 of <em>NATO ASI Series</em>, pages 133-165. Springer, Berlin, 1998.



</p>

<p id="press-teukolsky-vetterling-flannary-07">
W.&nbsp;H. Press, S.&nbsp;A. Teukolsky, W.&nbsp;T. Vetterling, and B.&nbsp;P. Flannary.
<a href="http://www.nr.com"><b>Numerical Recipes</b></a>.
Cambridge University Press, third edition, 2007.
Section 15.9.



</p>

<p id="rasmussen-williams-06">
C.&nbsp;E. Rasmussen and C.&nbsp;K.&nbsp;I. Williams.
<a href="http://www.gaussianprocess.org/gpml"><b>Gaussian Processes for Machine
  Learning</b></a>.
The MIT Press, Cambridge, MA, 2006.

<p class="c"><b> Comment:</b> The initial chapters contain significant amounts
  of tutorial material. The whole book, including all <a
  href="http://www.gaussianprocess.org/gpml/chapters">chapters</a> are freely
  available online.</p>

</p>

<p id="seeger-04">
M.&nbsp;Seeger.
<a href="http://www.kyb.tuebingen.mpg.de/bs/people/seeger/papers/bayesgp-tut.pdf"><b>Gaussian
  processes for machine learning</b></a>.
<em>International Journal of Neural Systems</em>, 14(2):69-106, 2004.<p
  class="c"><b> Abstract:</b> Gaussian processes (GPs) are natural
  generalisations of multivariate Gaussian random variables to infinite
  (countably or continuous) index sets. GPs have been applied in a large number
  of fields to a diverse range of ends, and very many deep theoretical analyses
  of various properties are available. This paper gives an introduction to
  Gaussian processes on a fairly elementary level with special emphasis on
  characteristics relevant in machine learning. It draws explicit connections
  to branches such as spline smoothing models and support vector machines in
  which similar ideas have been investigated.</p>



</p>

<p id="williams-02">
C.&nbsp;K.&nbsp;I. Williams.
<a href="http://www.dai.ed.ac.uk/homes/ckiw/postscript/hbtnn.ps.gz"><b>Gaussian
  processes</b></a>.
In M.&nbsp;A. Arbib, editor, <em>Handbook of Brain Theory and Neural Networks</em>,
  pages 466-470. The MIT Press, second edition, 2002.



</p>

<p id="williams-99">
C.&nbsp;K.&nbsp;I. Williams.
<a href="http://www.dai.ed.ac.uk/homes/ckiw/postscript/NCRG_97_012.ps.gz"><b>Prediction
  with Gaussian processes: From linear regression to linear prediction and
  beyond</b></a>.
In M.&nbsp;I. Jordan, editor, <em>Learning in Graphical Models</em>, pages 599-621.
  The MIT Press, Cambridge, MA, 1999.
Previously (1998) published by Kluwer Academic Press.<p class="c"><b>
  Abstract:</b> The main aim of this paper is to provide a tutorial on
  regression with Gaussian processes. We start from Bayesian linear regression,
  and show how by a change of viewpoint one can se this method as a Gaussian
  process predictor based on priors over functions, rather than on priors over
  parameters. This leads to a more general discussion of Gaussian processes in
  section 4. Section 5 deals with further issues, including hierarchical
  modelling and the setting of the parameters that control the Gaussian
  process, the covariance functions for neural network models and the use of
  Gaussian processes in classification problems.</p>



</p>
